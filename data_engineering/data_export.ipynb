{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc96cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/Workspace/fiap-datathon-decision/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "#import data_transform_utilities.flatten as flatten\n",
    "#from data_transform_utilities.text_parsers import  clean_str, extract_json, json_str_to_array, normalize_and_tokenize_text\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "import pysolr\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from sqlalchemy import create_engine, update, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_SHM_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/allan/Workspace/fiap-datathon-decision/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/allan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/allan/.ivy2/jars\n",
      "com.mysql#mysql-connector-j added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8a54e0b-c508-4a6e-9471-1a334283e446;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.mysql#mysql-connector-j;9.2.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;4.29.0 in central\n",
      ":: resolution report :: resolve 203ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;4.29.0 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;9.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8a54e0b-c508-4a6e-9471-1a334283e446\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "25/05/09 21:36:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.cores\", \"12\")\n",
    "spark_conf.set(\"spark.driver.cores\", \"12\")\n",
    "#spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.speculation\", False)\n",
    "spark_conf.set(\"spark.jars.packages\", \"com.mysql:mysql-connector-j:9.2.0\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder.master(\"local\") \\\n",
    "    .appName(\"Decision data overview\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6a420",
   "metadata": {},
   "source": [
    "# Extrai os dados do banco Relacional para Treinamento do Modelo e Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058833a",
   "metadata": {},
   "source": [
    "## Cria conexÃ£o com banco relacionsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801596cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://decision:1234@localhost/decision?charset=utf8\")\n",
    "days_to_read = 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9f95b",
   "metadata": {},
   "source": [
    "## Carrega os dados de vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 21:41:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://decision:1234@localhost:3306/decision?charset=utf8\",\n",
    "    table=f\"(SELECT * FROM vacancies WHERE requested_date > DATE_ADD(current_date(), INTERVAL -{days_to_read} DAY)) AS t\",\n",
    "    properties={\"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ").createOrReplaceTempView(\"vacancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9ab03",
   "metadata": {},
   "source": [
    "# Carrega os dados de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ddfa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://decision:1234@localhost:3306/decision?charset=utf8\",\n",
    "    table=f\"(SELECT * FROM applicants WHERE created_at > DATE_ADD(current_date(), INTERVAL -{days_to_read} DAY)) AS t\",\n",
    "    properties={\"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ").createOrReplaceTempView(\"applicants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011f44c",
   "metadata": {},
   "source": [
    "## Carrega os dados de candidatos que se canditaram a uma vaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a57897",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://decision:1234@localhost:3306/decision?charset=utf8\",\n",
    "    table=f\"(SELECT * FROM vacancies_applicants WHERE application_date > DATE_ADD(current_date(), INTERVAL -{days_to_read} DAY)) AS t\",\n",
    "    properties={\"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ").createOrReplaceTempView(\"vacancies_applicants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aae22",
   "metadata": {},
   "source": [
    "## Extrai apenas os campos textuais mais significativos para os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6645a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_location = \"../datasets/decision/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        v.id AS vacancy_id,\n",
    "        CONCAT(\n",
    "            COALESCE(main_activities, ''), '\\n', \n",
    "            COALESCE(technical_and_behavioral_skills, ''), '\\n',\n",
    "            COALESCE(behavioral_skills, '')\n",
    "        ) AS description,\n",
    "        CURRENT_DATE() AS dt\n",
    "    FROM\n",
    "        vacancies v\n",
    "\"\"\").repartition(1) \\\n",
    "    .write \\\n",
    "    .partitionBy('dt') \\\n",
    "    .option('header','true') \\\n",
    "    .option('sep',';') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(f'{storage_location}/export/vacancies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac689e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        a.id AS applicant_id,\n",
    "        CONCAT(\n",
    "            COALESCE(technical_knowledge, ''), '\\n',\n",
    "            COALESCE(cv_pt, '')\n",
    "        ) AS description,\n",
    "        CURRENT_DATE() AS dt\n",
    "    FROM\n",
    "        applicants a\n",
    "\"\"\").repartition(1). \\\n",
    "    write \\\n",
    "    .partitionBy('dt') \\\n",
    "    .option('header','true') \\\n",
    "    .option('sep',';') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(f'{storage_location}/export/applicants')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f4c69",
   "metadata": {},
   "source": [
    "## Cria o dataset para treino do Modelo com os dados que relaciona os melhores candidatos para as vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        (SELECT\n",
    "            va.id AS id,\n",
    "            CONCAT(\n",
    "                COALESCE(main_activities, ''), '\\n', \n",
    "                COALESCE(technical_and_behavioral_skills, ''), '\\n',\n",
    "                COALESCE(behavioral_skills, '')\n",
    "            ) AS vacancy_description,\n",
    "            CONCAT(\n",
    "                COALESCE(technical_knowledge, ''), '\\n',\n",
    "                COALESCE(cv_pt, '')\n",
    "            ) AS applicant_description,\n",
    "            IF( LOWER(status) LIKE '%contratado%', 1, 0 ) AS label,\n",
    "            CURRENT_DATE() AS dt\n",
    "        FROM \n",
    "            vacancies_applicants va\n",
    "            LEFT JOIN applicants a\n",
    "                ON va.applicant_id = a.id\n",
    "            LEFT JOIN vacancies v\n",
    "                ON va.vacancy_id = v.id\n",
    "        ) AS t\n",
    "    WHERE\n",
    "        LENGTH(TRIM(vacancy_description)) > 0\n",
    "        AND LENGTH(TRIM(applicant_description)) > 0\n",
    "    \n",
    "\"\"\").repartition(1). \\\n",
    "    write \\\n",
    "    .partitionBy('dt') \\\n",
    "    .option('header','true') \\\n",
    "    .option('sep',';') \\\n",
    "    .option('quoteAll','true') \\\n",
    "    .option('escapeQuotes', 'true') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(f'{storage_location}/train/vacancies_applicants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d83d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        REGEXP_REPLACE(vacancy_description, '\\n', ' ') AS vacancy_description,\n",
    "        REGEXP_REPLACE(applicant_description, '\\n', ' ') AS applicant_description,\n",
    "        label\n",
    "    FROM\n",
    "        (SELECT\n",
    "            va.id AS id,\n",
    "            CONCAT(\n",
    "                COALESCE(main_activities, ''), '\\n', \n",
    "                COALESCE(technical_and_behavioral_skills, ''), '\\n',\n",
    "                COALESCE(behavioral_skills, '')\n",
    "            ) AS vacancy_description,\n",
    "            CONCAT(\n",
    "                COALESCE(technical_knowledge, ''), '\\n',\n",
    "                COALESCE(cv_pt, '')\n",
    "            ) AS applicant_description,\n",
    "            IF( LOWER(status) LIKE '%contratado%', 1, 0 ) AS label,\n",
    "            CURRENT_DATE() AS dt\n",
    "        FROM \n",
    "            vacancies_applicants va\n",
    "            LEFT JOIN applicants a\n",
    "                ON va.applicant_id = a.id\n",
    "            LEFT JOIN vacancies v\n",
    "                ON va.vacancy_id = v.id\n",
    "        ) AS t\n",
    "    WHERE\n",
    "        LENGTH(TRIM(vacancy_description)) > 0\n",
    "        AND LENGTH(TRIM(applicant_description)) > 0\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3818fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a701134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17755d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        text=batch[\"vacancy_description\"],\n",
    "        text_pair=batch[\"applicant_description\"],\n",
    "        padding=\"max_length\", # garante que todos os exemplos do batch tenham o mesmo comprimento â exatamente o que o torch.stack espera.\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7cefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25567db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 17:07:16 WARN TransportChannelHandler: Exception in connection from fedora/192.168.101.88:35385\n",
      "java.io.IOException: Connection timed out\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:47)\n",
      "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:330)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:284)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:259)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:417)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff6505",
   "metadata": {},
   "source": [
    "TrainOutput(global_step=16902, training_loss=0.2363526517294551, metrics={'train_runtime': 10872.7876, 'train_samples_per_second': 12.436, 'train_steps_per_second': 1.555, 'total_flos': 3.557603512839168e+16, 'train_loss': 0.2363526517294551, 'epoch': 3.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"trained_model_bert_20250508\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4dea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_model_bert_20250508\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af8643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
