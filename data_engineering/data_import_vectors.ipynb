{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d6e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/Workspace/fiap-datathon-decision/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "#import data_transform_utilities.flatten as flatten\n",
    "#from data_transform_utilities.text_parsers import  clean_str, extract_json, json_str_to_array, normalize_and_tokenize_text\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "import pysolr\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine, update, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1975f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/allan/Workspace/fiap-datathon-decision/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/allan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/allan/.ivy2/jars\n",
      "com.mysql#mysql-connector-j added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dfa70fba-a845-429b-9f4c-4e561085fae6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.mysql#mysql-connector-j;9.2.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;4.29.0 in central\n",
      ":: resolution report :: resolve 138ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;4.29.0 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;9.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dfa70fba-a845-429b-9f4c-4e561085fae6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "25/05/10 13:53:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.cores\", \"12\")\n",
    "spark_conf.set(\"spark.driver.cores\", \"12\")\n",
    "spark_conf.set(\"spark.speculation\", False)\n",
    "spark_conf.set(\"spark.jars.packages\", \"com.mysql:mysql-connector-j:9.2.0\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder.master(\"local\") \\\n",
    "    .appName(\"Decision data overview\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a26c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://decision:1234@localhost/decision?charset=utf8\")\n",
    "days_to_read = 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4d2b0",
   "metadata": {},
   "source": [
    "## Carrega os dados de vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977cfac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 13:53:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://decision:1234@localhost:3306/decision?charset=utf8\",\n",
    "    table=f\"(SELECT * FROM vacancies WHERE requested_date > DATE_ADD(current_date(), INTERVAL -{days_to_read} DAY)) AS t\",\n",
    "    properties={\"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ").createOrReplaceTempView(\"vacancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c11b1",
   "metadata": {},
   "source": [
    "# Carrega os dados de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae5ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.jdbc(\n",
    "    url=\"jdbc:mysql://decision:1234@localhost:3306/decision?charset=utf8\",\n",
    "    table=f\"(SELECT * FROM applicants WHERE created_at > DATE_ADD(current_date(), INTERVAL -{days_to_read} DAY)) AS t\",\n",
    "    properties={\"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    ").createOrReplaceTempView(\"applicants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab36cb3",
   "metadata": {},
   "source": [
    "# Carrega o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43a18a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega modelo e tokenizer\n",
    "model_version = \"0.0.1\"\n",
    "#model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "model_name = \"../trained_model_bert_20250508\"\n",
    "tokenizer_name = \"../tokenizer_model_bert_20250508\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a096013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar embedding médio da sequência\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Média dos embeddings dos tokens (ignorando padding)\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(embeddings * mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "    mean_embedding = sum_embeddings / sum_mask\n",
    "    return mean_embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67c988",
   "metadata": {},
   "source": [
    "# Cria client do banco de Vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e5a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e577a",
   "metadata": {},
   "source": [
    "# Cria as coleções no Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e998d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(collection_name=\"applicants\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"applicants\",\n",
    "        vectors_config=VectorParams(size=model.config.hidden_size, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "if not client.collection_exists(collection_name=\"vacancies\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"vacancies\",\n",
    "        vectors_config=VectorParams(size=model.config.hidden_size, distance=Distance.COSINE),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753e238",
   "metadata": {},
   "source": [
    "# Inicia a inserção no banco de Vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa53287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_db(c, collection_name):\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=[\n",
    "            PointStruct(\n",
    "                id=c[\"id\"],\n",
    "                vector=c[\"embeddings\"],\n",
    "                payload={\"description\": c[\"description\"]}\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41356281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_batch(batch, collection_name, model, tokenizer):\n",
    "    result = [insert_into_db(\n",
    "        {\"id\": v.id, \"description\": v.description, \n",
    "        \"embeddings\": get_embedding(v.description, model, tokenizer), \n",
    "        \"model_version\": v.model_version}, collection_name) for v in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd462643",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        CONCAT(\n",
    "            COALESCE(main_activities, ''), '\\n', \n",
    "            COALESCE(technical_and_behavioral_skills, ''), '\\n',\n",
    "            COALESCE(behavioral_skills, ''), '\\n',\n",
    "            'país: ', COALESCE(country, ''), '\\n',\n",
    "            'estado: ', COALESCE(state, ''), '\\n',\n",
    "            'cidade: ', COALESCE(city, ''), '\\n'\n",
    "        ) AS description,\n",
    "        CURRENT_DATE() AS dt,\n",
    "        '{model_version}' AS model_version\n",
    "    FROM \n",
    "        vacancies v\n",
    "    ORDER BY id DESC\n",
    "    LIMIT 500\n",
    "\"\"\").collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48800e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_batch(vacancies, \"vacancies\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51874133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "applicants = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        CONCAT(\n",
    "            COALESCE(technical_knowledge, ''), '\\n',\n",
    "            COALESCE(cv_pt, ''), '\\n',\n",
    "            'Endereço: ', COALESCE(location, '')\n",
    "        ) AS description,\n",
    "        CURRENT_DATE() AS dt,\n",
    "        '{model_version}' AS model_version\n",
    "    FROM \n",
    "        applicants a\n",
    "    ORDER BY id DESC\n",
    "    LIMIT 500\n",
    "\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2806c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_batch(applicants, \"applicants\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a663b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import ExtendedPointId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67547fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ponto_a = client.retrieve(\n",
    "    collection_name=\"applicants\",\n",
    "    ids=[46068],\n",
    "    with_vectors=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "570c7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Recepção - Tutóia São Paulo\\nRecepção - Tutóia São Paulo\\n\\npaís: Brasil\\nestado: São Paulo\\ncidade: São Paulo\\n'}\n",
      "{'description': 'Analista de Processos Sênior\\nAnalista de Processos Sênior\\n\\npaís: Brasil\\nestado: São Paulo\\ncidade: Cajamar\\n'}\n"
     ]
    }
   ],
   "source": [
    "resultados = client.query_points(\n",
    "    collection_name=\"vacancies\",\n",
    "    query=ponto_a.vector, \n",
    "    limit=2\n",
    ")\n",
    "\n",
    "# Exibir resultados\n",
    "for r in resultados.points:\n",
    "    print(r.payload)\n",
    "    #print(f\"ID: {r.id} | Score: {r.score:.4f} | Descrição: {r.payload['descricao']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba206ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'description': 'Como administrador de banco de dados, você será responsável pela operação e suporte do banco de dados. Isso inclui atualizar designs de bancos de dados lógicos e físicos com alterações, gerar, configurar e otimizar o desempenho dos bancos de dados e fornecer informações sobre quaisquer decisões de design para interface ou integração de bancos de dados. Espera-se que você atue de forma independente e se torne um especialista no assunto (SME). É necessária participação ativa e contribuição nas discussões da equipe, além de fornecer soluções para problemas relacionados ao trabalho. É necessária proficiência especializada em administração do Exadata. Recomenda-se proficiência avançada em Oracle Database Administration (DBA), Database Administration e Oracle Database Cloud Services, bem como proficiência intermediária em Oracle Cloud Infrastructure (OCI) e Cloud Computing. Desenvolver e implementar estratégias de administração de banco de dados para garantir operações de banco de dados eficientes e confiáveis Monitorar e otimizar o desempenho do banco de dados, incluindo ajuste de consultas e otimização de configurações de banco de dados Solucionar e resolver problemas de banco de dados, incluindo gargalos de desempenho e problemas de integridade de dados Implementar e manter medidas de segurança de banco de dados, incluindo controles de acesso de usuários e criptografia de dados Colaborar com equipes multifuncionais para projetar e implementar soluções de banco de dados que atendam aos requisitos de negócios.\\nHibrido.\\n\\npaís: Brasil\\nestado: São Paulo\\ncidade: São Paulo\\n'}\n",
    "{'description': '06 meses iniciais de projeto\\nAtuação Hibrido, (maior parte vai ser remota com algumas poucas visitas ao cliente na cidade de São Paulo )\\nProfissional com base São Paulo. Tem que ser profissionais que residam em São Paulo\\n\\nO que você fará?\\n• Participar de todas as implementações do SAP for Insurance, desde a concepção da solução (blueprint) até as atividades de go-live e pós-go-live\\n• Criação de especificações funcionais, fluxos de processo, customização, treinamento e outros documentos relacionados ao projeto\\n• Realização da personalização do sistema SAP e verificação da qualidade da implementação\\n• Resolver problemas específicos do módulo como pessoa de contato para o cliente relacionado ao setor financeiro (tópicos relacionados ao FS-CD)\\n• Criar documentação padrão de negócios para todos os novos desenvolvimentos\\n• Realizar execuções de testes (testes unitários, testes de integração do sistema) e dar suporte aos usuários nos ciclos de testes de aceitação do usuário.\\n• Treinar e auxiliar os colegas mais jovens\\n• Ser membro ativo da comunidade de consultoria, compartilhando conhecimento e orientando/treinando consultores juniores\\nConhecimentos, habilidades e aptidões necessários:\\n• Experiência profissional em SAP de mais de 1 ano na área de FS-CD, FI-CA ou solução similar (IS-T, IS-U, IS-M, IS-PS-CA (PSCD))\\n• Diploma de ensino superior relevante (Economia, Tecnologia da Informação ou equivalente)\\n• Conhecimento de processos financeiros e contábeis\\n• Capacidade de traduzir requisitos comerciais em especificações de projeto de sistema\\n• Eficiente e comunicativo, com espírito de equipe e iniciativa própria\\n• Base São Paulo, mas disponibilidade para viajar\\n• Excelente domínio do idioma inglês\\n\\nRequisitos desejáveis:\\n• Conhecimento das soluções de ERP, como S4/HANA, SAP FI-CO, FI-AM, FI-GL, FI-AR. FI-AP\\n• Experiência em consultoria em SAP ou outras implementações de ERP\\n• Experiência em liderança de projeto (liderança de equipe)\\n• Familiarizado com as metodologias de projeto SCRUM, PRINCE 2 ou ASAP\\n• Conhecimento de IFR17 ou IFRS9\\n\\npaís: Brasil\\nestado: São Paulo\\ncidade: São Paulo\\n'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
