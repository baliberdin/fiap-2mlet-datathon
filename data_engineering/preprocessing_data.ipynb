{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import data_transform_utilities.flatten as flatten\n",
    "from data_transform_utilities.text_parsers import  clean_str, extract_json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import pysolr\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria a sessão do Spark (Nesessário JVM e a variável de ambiente JAVA_HOME definida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.cores\", \"8\")\n",
    "spark_conf.set(\"spark.driver.cores\", \"8\")\n",
    "spark_conf.set(\"spark.jars.packages\", \"com.mysql:mysql-connector-j:9.2.0\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder.master(\"local\") \\\n",
    "    .appName(\"Decision data overview\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registra as UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"clean_str\", clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria engine para conexão com o MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://decision:1234@localhost/decision?charset=utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lê os dados crú do disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(dataset_name: str):\n",
    "    dataset = pd.read_json(f'../datasets/{dataset_name}.json', orient='index', typ='frame', encoding='UTF-8')\n",
    "    dataset['id'] = dataset.index\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy = read_raw_data(\"vagas\")\n",
    "applicants = read_raw_data(\"applicants\")\n",
    "prospects = read_raw_data(\"prospects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy = flatten(vacancy)\n",
    "applicants = flatten(applicants)\n",
    "prospects = prospects.explode(\"prospects\")\n",
    "prospects = flatten(prospects).drop(columns=\"prospects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "applicants.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "prospects.replace(r'^\\s*$', None, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy.to_parquet(f'../datasets/parquet/vacancy.parquet')\n",
    "applicants.to_parquet(f'../datasets/parquet/applicants.parquet')\n",
    "prospects.to_parquet(f'../datasets/parquet/prospects.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(f'../datasets/parquet/vacancy.parquet').createOrReplaceTempView(\"vacancy\")\n",
    "spark.read.parquet(f'../datasets/parquet/applicants.parquet').createOrReplaceTempView(\"applicants\")\n",
    "spark.read.parquet(f'../datasets/parquet/prospects.parquet').createOrReplaceTempView(\"prospects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from vacancy\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from prospects\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from applicants\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos dados disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select count(distinct id) as total, count(1) as lines from prospects\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select count(distinct id) as total, count(1) as lines from applicants\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select count(distinct id) as total, count(1) as lines from vacancy\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        count(distinct id) AS total_vacancy,\n",
    "        avg(prospects) AS avg_prospects,\n",
    "        min(prospects) AS min_prospects,\n",
    "        max(prospects) AS max_prospects,\n",
    "        count(if(prospects = 0, 1, null)) AS total_vacancy_without_prospects\n",
    "    FROM\n",
    "        (SELECT \n",
    "            id,\n",
    "            count(prospects_nome) AS prospects\n",
    "        FROM prospects\n",
    "        GROUP BY 1\n",
    "        ) AS t\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "#     SELECT\n",
    "#         v.id,\n",
    "#         v.informacoes_basicas_titulo_vaga,\n",
    "#         v.informacoes_basicas_tipo_contratacao,\n",
    "#         p.prospects_codigo,\n",
    "#         p.prospects_nome,\n",
    "#         p.prospects_situacao_candidado,\n",
    "#         a.infos_basicas_codigo_profissional,\n",
    "#         a.informacoes_pessoais_data_nascimento,\n",
    "#         a.informacoes_profissionais_titulo_profissional\n",
    "#         \n",
    "#         -- a.infos_basicas_nome\n",
    "#     FROM\n",
    "#         prospects p\n",
    "#         LEFT JOIN vacancy v ON v.id = p.id\n",
    "#         LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "#     WHERE\n",
    "#         v.id = 10975\n",
    "#         -- AND prospects_codigo = 41496\n",
    "# \"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(f\"\"\"\n",
    "#    SELECT\n",
    "#    *\n",
    "#    FROM\n",
    "#        prospects\n",
    "#    WHERE\n",
    "#        prospects_codigo = 41485\n",
    "#\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "#     SELECT\n",
    "#         v.id as v_id,\n",
    "#         p.id as p_id,\n",
    "#         a.id as a_id,\n",
    "#         p.prospects_codigo,\n",
    "#         v.informacoes_basicas_titulo_vaga,\n",
    "#         p.prospects_nome,\n",
    "#         a.infos_basicas_nome,\n",
    "#         prospects_situacao_candidado\n",
    "#     FROM\n",
    "#         prospects p\n",
    "#         LEFT JOIN vacancy v ON v.id = p.id\n",
    "#         LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "#     WHERE\n",
    "#         -- v.id = 10976\n",
    "#         -- AND prospects_codigo = 41496\n",
    "#         -- AND \n",
    "#         -- a.id = 41496\n",
    "#         LOWER(p.prospects_situacao_candidado) like '%contratado%'\n",
    "# \"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "#     SELECT\n",
    "#         v.id as v_id,\n",
    "#         p.id as p_id,\n",
    "#         a.id as a_id,\n",
    "#         p.prospects_codigo,\n",
    "#         v.informacoes_basicas_titulo_vaga,\n",
    "#         p.prospects_nome,\n",
    "#         a.infos_basicas_nome\n",
    "#     FROM\n",
    "#         prospects p\n",
    "#         LEFT JOIN vacancy v ON v.id = p.id\n",
    "#         LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "#     WHERE\n",
    "#         -- v.id = 10975\n",
    "#         -- AND prospects_codigo = 41496\n",
    "#         -- AND \n",
    "#         a.id = 41496\n",
    "# \"\"\").drop(\"id\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação de relação entre as tabelas\n",
    "Estou assumindo que a tabela porspects é uma tabela de JOIN NxN entre as tabelas vacancy e applicants onde temos N vagas para N candidatos.\n",
    "Nessa situação não deveríamos ter prospects com ID de vaga que não existe na tabela de vagas e nem ID de candidado que não exista na tabela de candidatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        count(1) as lines,\n",
    "        count(distinct p.id) as distinct_prospects,\n",
    "        count(distinct v.id) distinct_vacancies,\n",
    "        count(distinct a.id) as distinct_applicants,\n",
    "        count(distinct p.prospects_codigo) as distinct_prospects_codes,\n",
    "        count(if(a.id is null, 1, null)) as prospects_without_applicants,\n",
    "        count(if(v.id is null, 1, null)) as prospects_without_vacancies\n",
    "    FROM\n",
    "        prospects p\n",
    "        LEFT JOIN vacancy v ON v.id = p.id\n",
    "        LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acima percebemos que há inconsistência nas relações das tabelas. Existem prospects que apontam para vagas que não existem e prospects que apontam para candidatos que não existem.\n",
    "## E abaixo vemos que náo há duplicidades nos prospects, ou seja, não temos um candidato associado mais de uma vez a uma vaga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        prospect_id,\n",
    "        count(1) as total\n",
    "    FROM\n",
    "        (SELECT\n",
    "            a.id || '_' || v.id as prospect_id\n",
    "        FROM\n",
    "            prospects p\n",
    "            LEFT JOIN vacancy v ON v.id = p.id\n",
    "            LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "        WHERE\n",
    "            v.id IS NOT NULL AND a.id IS NOT NULL\n",
    "        ) as t\n",
    "    GROUP BY 1\n",
    "    HAVING total > 1\n",
    "    ORDER by total desc\n",
    "        \n",
    "\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual a média de vagas associadas a um candidatos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        count(distinct applicant_id) applicatns,\n",
    "        avg(vacancies) as avg_vacancies,\n",
    "        min(vacancies) as min_vacancies,\n",
    "        max(vacancies) as max_vacancies,\n",
    "        count(1) as lines\n",
    "    FROM\n",
    "        (SELECT\n",
    "            a.id as applicant_id,\n",
    "            count(distinct v.id) as vacancies,\n",
    "            count(1) as lines\n",
    "        FROM\n",
    "            prospects p\n",
    "            LEFT JOIN vacancy v ON v.id = p.id\n",
    "            LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "        WHERE\n",
    "            v.id IS NOT NULL AND a.id IS NOT NULL\n",
    "        GROUP BY 1\n",
    "        ) as t\n",
    "        \n",
    "\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual a distribuição por faixas de quantidade de candidatura de candidatos? Quantos candidatos estão em apenas 1 vaga ou em 2 e assim por diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        applicatns * 100 / agg_total as percent\n",
    "    FROM\n",
    "        (SELECT\n",
    "            CASE \n",
    "                WHEN vacancies <= 2  THEN 2\n",
    "                WHEN vacancies <= 5  THEN 5\n",
    "                WHEN vacancies <= 10  THEN 10\n",
    "                WHEN vacancies <= 20  THEN 20\n",
    "                WHEN vacancies <= 30  THEN 30\n",
    "                WHEN vacancies <= 50  THEN 50\n",
    "                WHEN vacancies <= 80  THEN 80\n",
    "                ELSE 81\n",
    "            END as range,\n",
    "            count(distinct applicant_id) applicatns,\n",
    "            SUM(count(1)) OVER () as agg_total\n",
    "        FROM\n",
    "            (SELECT\n",
    "                a.id as applicant_id,\n",
    "                count(distinct v.id) as vacancies,\n",
    "                count(1) as lines\n",
    "            FROM\n",
    "                prospects p\n",
    "                LEFT JOIN vacancy v ON v.id = p.id\n",
    "                LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "            WHERE\n",
    "                v.id IS NOT NULL AND a.id IS NOT NULL -- Linpa a base para não contar os prospectos que não tem vaga ou candidato\n",
    "            GROUP BY 1\n",
    "            ) as t\n",
    "        GROUP BY 1\n",
    "        ORDER BY 1\n",
    "        ) as t   \n",
    "\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        prospects p\n",
    "        LEFT JOIN vacancy v ON v.id = p.id\n",
    "        LEFT JOIN applicants a ON a.id = p.prospects_codigo\n",
    "    WHERE\n",
    "        v.id IS NOT NULL AND a.id IS NOT NULL\n",
    "\"\"\").drop(\"id\").createOrReplaceTempView(\"tmp_clean_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tmp_clean_data\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        perfil_vaga_competencia_tecnicas_e_comportamentais,\n",
    "        perfil_vaga_habilidades_comportamentais_necessarias\n",
    "    from tmp_clean_data\n",
    "    limit 1\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega dados de Vagas para a base relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    select\n",
    "        TO_DATE(informacoes_basicas_data_requicisao, 'dd-MM-yyyy') AS requested_date,\n",
    "        TO_DATE(informacoes_basicas_limite_esperado_para_contratacao, 'dd-MM-yyyy' ) AS expected_hiring_date,\n",
    "        informacoes_basicas_titulo_vaga AS title,\n",
    "        CLEAN_STR(informacoes_basicas_titulo_vaga) AS normalized_title,\n",
    "        CASE \n",
    "            WHEN LOWER(TRIM(informacoes_basicas_vaga_sap)) = 'não' THEN false\n",
    "            WHEN LOWER(TRIM(informacoes_basicas_vaga_sap)) = 'sim' THEN true\n",
    "        ELSE NULL \n",
    "        END AS sap_job,\n",
    "        \n",
    "        informacoes_basicas_cliente AS client,\n",
    "        informacoes_basicas_solicitante_cliente AS client_requester,\n",
    "        informacoes_basicas_empresa_divisao AS department,\n",
    "        informacoes_basicas_requisitante AS requester,\n",
    "        \n",
    "        informacoes_basicas_analista_responsavel AS responsible_analyst,\n",
    "        -- SPLIT(informacoes_basicas_tipo_contratacao, ',') AS hiring_type,\n",
    "        informacoes_basicas_tipo_contratacao AS hiring_type,\n",
    "        informacoes_basicas_prazo_contratacao AS hiring_deadline,\n",
    "        informacoes_basicas_objetivo_vaga AS objective,\n",
    "        TRIM(SPLIT_PART(informacoes_basicas_prioridade_vaga, ':', 1)) AS priority,\n",
    "        informacoes_basicas_origem_vaga AS reason,\n",
    "        informacoes_basicas_superior_imediato AS manager,\n",
    "        informacoes_basicas_nome AS name,\n",
    "        informacoes_basicas_telefone AS phone_number,\n",
    "        perfil_vaga_pais AS country,\n",
    "        perfil_vaga_estado AS state,\n",
    "        perfil_vaga_cidade AS city,\n",
    "        perfil_vaga_bairro AS neighborhood,\n",
    "        perfil_vaga_regiao AS region,\n",
    "        perfil_vaga_local_trabalho AS workplace,\n",
    "        CASE \n",
    "            WHEN LOWER(TRIM(perfil_vaga_vaga_especifica_para_pcd)) = 'não' THEN false \n",
    "            WHEN LOWER(TRIM(perfil_vaga_vaga_especifica_para_pcd)) = 'sim' THEN true\n",
    "            ELSE NULL\n",
    "        END AS only_pwd,\n",
    "        perfil_vaga_faixa_etaria AS age_range,\n",
    "        perfil_vaga_horario_trabalho AS work_schedule,\n",
    "        `perfil_vaga_nivel profissional` AS professional_level,\n",
    "        perfil_vaga_nivel_academico AS academic_level,\n",
    "        perfil_vaga_nivel_ingles AS english_level,\n",
    "        perfil_vaga_nivel_espanhol AS spanish_level,\n",
    "        perfil_vaga_outro_idioma AS other_language,\n",
    "        -- ARRAY_DISTINCT(\n",
    "        --     FILTER(\n",
    "        --         TRANSFORM(\n",
    "        --             SPLIT(perfil_vaga_areas_atuacao, '-'), x -> TRIM(x)\n",
    "        --         ), \n",
    "        --         x -> LENGTH(x) > 0 \n",
    "        --     )\n",
    "        -- ) AS areas_of_expertise,\n",
    "        \n",
    "        ARRAY_JOIN(ARRAY_DISTINCT(\n",
    "            FILTER(\n",
    "                TRANSFORM(\n",
    "                    SPLIT(perfil_vaga_areas_atuacao, '-'), x -> TRIM(x)\n",
    "                ), \n",
    "                x -> LENGTH(x) > 0 \n",
    "            )\n",
    "        ), ',') AS areas_of_expertise,\n",
    "        \n",
    "        perfil_vaga_principais_atividades AS main_activities,\n",
    "        perfil_vaga_competencia_tecnicas_e_comportamentais AS technical_and_behavioral_skills,\n",
    "        perfil_vaga_demais_observacoes AS other_observations,\n",
    "        CASE \n",
    "            WHEN LOWER(TRIM(perfil_vaga_viagens_requeridas)) = 'não' THEN false\n",
    "            WHEN LOWER(TRIM(perfil_vaga_viagens_requeridas)) = 'sim' THEN true\n",
    "        ELSE null\n",
    "        END AS required_travels,\n",
    "        perfil_vaga_equipamentos_necessarios AS required_equipment,\n",
    "        beneficios_valor_venda AS selling_value,\n",
    "        beneficios_valor_compra_1 AS purchase_value_1,\n",
    "        beneficios_valor_compra_2 AS purchase_value_2,\n",
    "        id,\n",
    "        TO_DATE(informacoes_basicas_data_inicial, 'dd-MM-yyyy') AS start_date,\n",
    "        TO_DATE(informacoes_basicas_data_final, 'dd-MM-yyyy') AS end_date,\n",
    "        perfil_vaga_habilidades_comportamentais_necessarias AS behavioral_skills,\n",
    "        informacoes_basicas_nome_substituto AS substitute_name\n",
    "    from vacancy\n",
    "    -- ORDER BY id desc\n",
    "\"\"\").createOrReplaceTempView(\"tmp_transformed_vacancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT * FROM tmp_transformed_vacancy\"\"\").write.jdbc( \\\n",
    "    url=\"jdbc:mysql://localhost:3306/decision\", \\\n",
    "    table=\"vacancies\", \\\n",
    "    mode=\"append\", \\\n",
    "    properties={\"driver\":\"com.mysql.jdbc.Driver\", \"user\":\"decision\", \"password\":\"1234\"} \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega dados de candidadtos para a base Relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        infos_basicas_objetivo_profissional AS professional_objective,\n",
    "        TO_TIMESTAMP(infos_basicas_data_criacao, 'dd-MM-yyyy HH:mm:ss') AS created_at,\n",
    "        infos_basicas_inserido_por AS inserted_by,\n",
    "        \n",
    "        TO_TIMESTAMP(infos_basicas_data_atualizacao, 'dd-MM-yyyy HH:mm:ss') AS updated_at,\n",
    "        infos_basicas_codigo_profissional AS professional_code,\n",
    "        TO_TIMESTAMP(informacoes_pessoais_data_aceite, 'dd/MM/yyyy HH:mm') AS acceptance_date,\n",
    "        \n",
    "        -- Verificar se os dados não se diferem\n",
    "        infos_basicas_nome AS name,\n",
    "        -- informacoes_pessoais_nome AS name2,\n",
    "        \n",
    "        -- Verificar se não é só null\n",
    "        informacoes_pessoais_cpf AS cpf,\n",
    "        \n",
    "        -- Um é o valor encurtado e o outro o valor completo inserido pelo candidato\n",
    "        infos_basicas_sabendo_de_nos_por AS source,\n",
    "        -- informacoes_pessoais_fonte_indicacao AS source_text,\n",
    "        \n",
    "        -- Verificar se são iguais\n",
    "        infos_basicas_email AS email,\n",
    "        -- informacoes_pessoais_email AS email2,\n",
    "        \n",
    "        -- Verificar se não é só null\n",
    "        informacoes_pessoais_email_secundario AS secondary_email,\n",
    "        TO_DATE(informacoes_pessoais_data_nascimento, 'dd-MM-yyyy') AS birth_date,\n",
    "        \n",
    "        -- Verificar se não é só null e se não são iguais\n",
    "        infos_basicas_telefone_recado AS secondary_phone_number,\n",
    "        -- informacoes_pessoais_telefone_recado AS secondary_phone_number,\n",
    "        \n",
    "        -- Verificar se não são iguais\n",
    "        infos_basicas_telefone AS phone_number,\n",
    "        informacoes_pessoais_telefone_celular AS cellphone,\n",
    "        \n",
    "        informacoes_pessoais_sexo AS gender,\n",
    "        informacoes_pessoais_estado_civil AS marital_status,\n",
    "        CASE \n",
    "            WHEN TRIM(LOWER(informacoes_pessoais_pcd)) = 'não' THEN false\n",
    "            WHEN TRIM(LOWER(informacoes_pessoais_pcd)) = 'nao' THEN false\n",
    "            WHEN TRIM(LOWER(informacoes_pessoais_pcd)) = 'sim' THEN true \n",
    "            ELSE NULL\n",
    "        END AS is_pwd,\n",
    "        \n",
    "        -- São muito parecidos, mas o location contem cidade e estado\n",
    "        infos_basicas_local AS location,\n",
    "        -- informacoes_pessoais_endereco AS address,\n",
    "        \n",
    "        -- Verificar se não é tudo nulo\n",
    "        informacoes_pessoais_skype AS skype,\n",
    "        \n",
    "        -- Verificar se não é tudo nulo\n",
    "        informacoes_pessoais_url_linkedin AS linkedin_url,\n",
    "        \n",
    "        -- Verificar se não é tudo nulo\n",
    "        informacoes_pessoais_facebook AS facebook_url,\n",
    "        \n",
    "        informacoes_profissionais_titulo_profissional AS professional_title,\n",
    "        informacoes_profissionais_area_atuacao AS area_of_expertise,\n",
    "        \n",
    "        -- Verificar se não é tudo nulo\n",
    "        informacoes_profissionais_conhecimentos_tecnicos AS technical_knowledge,\n",
    "        \n",
    "        -- Verificar se não é tudo nulo\n",
    "        informacoes_profissionais_certificacoes AS certifications,\n",
    "        informacoes_profissionais_outras_certificacoes AS other_certifications,\n",
    "        \n",
    "        \n",
    "        informacoes_profissionais_remuneracao AS salary, \n",
    "        informacoes_profissionais_nivel_profissional AS professional_level,\n",
    "        formacao_e_idiomas_nivel_academico AS academic_level,\n",
    "        formacao_e_idiomas_nivel_ingles AS english_level,\n",
    "        formacao_e_idiomas_nivel_espanhol AS spanish_level,\n",
    "        IF(TRIM(formacao_e_idiomas_outro_idioma) = '-', null, formacao_e_idiomas_outro_idioma) AS other_language,\n",
    "        \n",
    "        cv_pt AS cv_pt,\n",
    "        -- verificar se não é tudo nulo\n",
    "        cv_en,\n",
    "        formacao_e_idiomas_instituicao_ensino_superior AS higher_education_institution,\n",
    "        \n",
    "        -- Parece conter outros cursos que não são de idiomas\n",
    "        formacao_e_idiomas_cursos AS language_courses,\n",
    "        formacao_e_idiomas_ano_conclusao AS language_courses_year,\n",
    "        \n",
    "        -- Verificar se todos não são somente nulos\n",
    "        informacoes_pessoais_download_cv AS cv_filename,\n",
    "        informacoes_profissionais_qualificacoes AS qualifications,\n",
    "        informacoes_profissionais_experiencias AS experiences,\n",
    "        formacao_e_idiomas_outro_curso AS other_courses,\n",
    "        \n",
    "        -- Verificar se não é nulo\n",
    "        cargo_atual_id_ibrati AS current_job_id,\n",
    "        cargo_atual_email_corporativo AS corporate_email,\n",
    "        cargo_atual_cargo_atual AS current_job,\n",
    "        cargo_atual_projeto_atual AS current_job_project,\n",
    "        cargo_atual_cliente AS current_job_client,\n",
    "        cargo_atual_unidade AS current_job_unit,\n",
    "        cargo_atual_data_admissao AS current_job_admission_date,\n",
    "        cargo_atual_data_ultima_promocao AS current_job_last_promotion_date,\n",
    "        cargo_atual_nome_superior_imediato AS current_job_immediate_superior_name,\n",
    "        cargo_atual_email_superior_imediato AS current_job_immediate_superior_email\n",
    "    from applicants\n",
    "    -- ORDER BY id desc\n",
    "\"\"\").createOrReplaceTempView(\"tmp_transformed_applicants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT * FROM tmp_transformed_applicants\"\"\").write.jdbc( \\\n",
    "    url=\"jdbc:mysql://localhost:3306/decision\", \\\n",
    "    table=\"applicants\", \\\n",
    "    mode=\"append\", \\\n",
    "    properties={\"driver\":\"com.mysql.jdbc.Driver\", \"user\":\"decision\", \"password\":\"1234\"} \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega os dados de candidaturas no banco relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        RANK() OVER (PARTITION BY 1 ORDER BY p.id, p.prospects_codigo) AS id,\n",
    "        id AS vacancy_id,\n",
    "        prospects_codigo AS applicant_id,\n",
    "        prospects_situacao_candidado AS status,\n",
    "        TO_DATE(prospects_data_candidatura, 'dd-MM-yyyy') AS application_date,\n",
    "        TO_DATE(prospects_ultima_atualizacao, 'dd-MM-yyyy') AS last_update,\n",
    "        -- prospects_comentario AS comment,\n",
    "        prospects_recrutador AS recruiter\n",
    "        \n",
    "    -- titulo: string (nullable = true)\n",
    "    -- modalidade: string (nullable = true)\n",
    "    -- prospects_nome: string (nullable = true)\n",
    "    -- prospects_codigo: string (nullable = true)\n",
    "    -- prospects_situacao_candidado: string (nullable = true)\n",
    "    -- prospects_data_candidatura: string (nullable = true)\n",
    "    -- prospects_ultima_atualizacao: string (nullable = true)\n",
    "    -- prospects_comentario: string (nullable = true)\n",
    "    -- prospects_recrutador: string (nullable = true)\n",
    "    -- id: long (nullable = true)\n",
    "    FROM\n",
    "        prospects p\n",
    "    WHERE\n",
    "        id IN (SELECT id FROM vacancy)\n",
    "        AND prospects_codigo IN (SELECT id FROM applicants)\n",
    "\"\"\").createOrReplaceTempView(\"tmp_transformed_prospects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM tmp_transformed_prospects          \n",
    "\"\"\").write.jdbc( \\\n",
    "    url=\"jdbc:mysql://localhost:3306/decision\", \\\n",
    "    table=\"vacancies_applicants\", \\\n",
    "    mode=\"append\", \\\n",
    "    properties={\"driver\":\"com.mysql.jdbc.Driver\", \"user\":\"decision\", \"password\":\"1234\"} \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria dataset para treino do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        vacancy_id,\n",
    "        count(distinct applicant_id) as total_applicants\n",
    "    FROM\n",
    "        tmp_transformed_prospects\n",
    "    WHERE\n",
    "        id IN (SELECT id FROM vacancy)\n",
    "        AND applicant_id IN (SELECT id FROM applicants)\n",
    "        AND LOWER(status) LIKE '%contratado%'\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "\"\"\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        total_applicants,\n",
    "        count(distinct vacancy_id) as total_vacancies,\n",
    "        total_applicants * count(distinct vacancy_id) as lines\n",
    "    FROM\n",
    "        (SELECT\n",
    "            vacancy_id,\n",
    "            count(distinct IF( LOWER(status) LIKE '%contratado%', applicant_id, NULL)) as total_applicants,\n",
    "            COUNT(1) as lines\n",
    "        FROM\n",
    "            tmp_transformed_prospects\n",
    "        WHERE\n",
    "            id IN (SELECT id FROM vacancy)\n",
    "            AND applicant_id IN (SELECT id FROM applicants)\n",
    "            \n",
    "        GROUP BY 1\n",
    "        ORDER BY 2 DESC\n",
    "        ) AS t\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1 DESC\n",
    "\"\"\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from data_transform_utilities.text_parsers import clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"clean_str\", clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT clean_str('100-200 - Test') as title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"select clean_str('consultant operations - ') as title\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    select \n",
    "        normalized_title as title,\n",
    "        count(distinct id) as vacancies \n",
    "    from \n",
    "        tmp_transformed_vacancy  \n",
    "    group by \n",
    "        1 \n",
    "    order by \n",
    "        2 desc\n",
    "\"\"\").createOrReplaceTempView(\"tmp_normalized_titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tmp_normalized_titles order by vacancies desc\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        vacancies, \n",
    "        count(distinct title) as titles\n",
    "    from tmp_normalized_titles \n",
    "    group by 1 \n",
    "    order by 1 desc\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        CASE\n",
    "            WHEN vacancies >= 20 THEN '20+'\n",
    "            WHEN vacancies >= 10 THEN '10-20'\n",
    "            WHEN vacancies >= 5  THEN '05-10'\n",
    "            WHEN vacancies >= 2  THEN '02-05'\n",
    "            ELSE '01'\n",
    "        END as vacancies, \n",
    "        SUM(vacancies) as total_vacancies,\n",
    "        count(distinct title) as titles,\n",
    "        SUM(vacancies) / count(distinct title) as avg_vacancies,\n",
    "        MIN(vacancies) as min_vacancies,\n",
    "        MAX(vacancies) as max_vacancies\n",
    "    from \n",
    "        tmp_normalized_titles \n",
    "    group by 1 \n",
    "    order by 1 desc\n",
    "\"\"\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        CASE\n",
    "            WHEN vacancies >= 20 THEN '20+'\n",
    "            WHEN vacancies >= 10 THEN '10-20'\n",
    "            WHEN vacancies >= 5  THEN '05-10'\n",
    "            WHEN vacancies >= 2  THEN '02-05'\n",
    "            ELSE '01'\n",
    "        END as vacancies, \n",
    "        SUM(vacancies) as total_vacancies,\n",
    "        count(distinct title) as titles,\n",
    "        SUM(vacancies) / count(distinct title) as avg_vacancies,\n",
    "        MIN(vacancies) as min_vacancies,\n",
    "        MAX(vacancies) as max_vacancies\n",
    "    from \n",
    "        tmp_normalized_titles \n",
    "    group by 1 \n",
    "    order by 1 desc\n",
    "\"\"\").show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexação no Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysolr\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solr = pysolr.Solr('http://localhost:8983/solr/vacancies') # Replace with your Solr URL and core name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_index(collection):\n",
    "    solr = pysolr.Solr(f'http://localhost:8983/solr/{collection}')\n",
    "    solr.delete(q='*:*')\n",
    "    solr.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_index('vacancies')\n",
    "truncate_index('applicants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://decision:1234@localhost/decision?charset=utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vacancy_chunck(conn, page, page_size):\n",
    "    return pd.read_sql_query(f\"\"\"\n",
    "        SELECT\n",
    "            id,\n",
    "            title,\n",
    "            normalized_title,\n",
    "            main_activities,\n",
    "            technical_and_behavioral_skills,\n",
    "            behavioral_skills,\n",
    "            state,\n",
    "            city,\n",
    "            country\n",
    "        FROM\n",
    "            vacancies\n",
    "        limit  {page},{page_size}\n",
    "    \"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_applicants_chunck(conn, page, page_size):\n",
    "    return pd.read_sql_query(f\"\"\"\n",
    "        SELECT\n",
    "            id,\n",
    "            professional_objective,\n",
    "            professional_title,\n",
    "            name,\n",
    "            email,\n",
    "            -- birth_date,\n",
    "            gender,\n",
    "            is_pwd,\n",
    "            marital_status,\n",
    "            technical_knowledge,\n",
    "            academic_level,\n",
    "            -- salary,\n",
    "            english_level,\n",
    "            spanish_level,\n",
    "            cv_pt,\n",
    "            higher_education_institution,\n",
    "            language_courses,\n",
    "            location\n",
    "        FROM\n",
    "            applicants\n",
    "        limit  {page},{page_size}\n",
    "    \"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(conn, collection, documents_fn, page_size=10):\n",
    "    solr = pysolr.Solr(f'http://localhost:8983/solr/{collection}')\n",
    "    with engine.connect() as conn:\n",
    "        page = 0\n",
    "        result = documents_fn(conn, page, page_size)\n",
    "        \n",
    "        while len(result) > 0:\n",
    "            data = json.loads(result.to_json(orient='records'))\n",
    "\n",
    "            try:\n",
    "                solr.add(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing data: {e}\")\n",
    "            \n",
    "            page = page + page_size\n",
    "            result = documents_fn(conn, page, page_size)\n",
    "\n",
    "    solr.commit()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()\n",
    "index_documents(conn, 'vacancies', read_vacancy_chunck, 50)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()\n",
    "index_documents(conn, 'applicants', read_applicants_chunck, 1000)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usa LLM para extrair os requisitos das Vagas e as habilidades dos candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_api_endpoint = 'http://192.168.101.186:1234/v1/chat/completions'\n",
    "llm_api_headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Host': 'localhost'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega apenas as vagas que tiveram candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()\n",
    "db_vacancies = pd.read_sql_query(f\"\"\"\n",
    "        SELECT\n",
    "            id,\n",
    "            main_activities,\n",
    "            technical_and_behavioral_skills,\n",
    "            behavioral_skills\n",
    "        FROM\n",
    "            vacancies\n",
    "        WHERE\n",
    "            id IN\n",
    "             (SELECT vacancy_id FROM vacancies_applicants WHERE applicant_id IS NOT NULL)\n",
    "        -- ORDER BY id\n",
    "    \"\"\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, update, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vacancy_llm_result(id :int, llm_result :str, engine):\n",
    "    metadata = MetaData()\n",
    "    vacancy_table = Table('vacancies', metadata, autoload_with=engine)\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "    # Start a session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    # Update a row\n",
    "    stmt = update(vacancy_table).where(vacancy_table.c.id == id).values(llm_result=llm_result)\n",
    "    session.execute(stmt)\n",
    "    session.commit()\n",
    "\n",
    "    # Close the session\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "with open('../agents/vacancies_json_extraction.txt', 'r') as f:\n",
    "    prompt = f.read()\n",
    "        \n",
    "for l,v in db_vacancies.iterrows():\n",
    "    vacancy_description = f\"\"\"\n",
    "    {v['main_activities']}\n",
    "    {v['technical_and_behavioral_skills']}  \n",
    "    {v['behavioral_skills']}\n",
    "    \"\"\"\n",
    "    \n",
    "    #result = \"\"\n",
    "    vacancy_prompt = prompt.replace('${VACANCY_DESCRIPTION}', vacancy_description)\n",
    "    result = requests.post(llm_api_endpoint, headers=llm_api_headers, json={\"messages\":[{\"role\":\"user\",\"content\":vacancy_prompt}]})\n",
    "    \n",
    "    try:\n",
    "        response = result.json().get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "        str_json = extract_json(response)\n",
    "        response = str_json\n",
    "\n",
    "        print(f\"Updating Vacancy ID: {v['id']}\")\n",
    "        update_vacancy_llm_result(v['id'], json.dumps(response), engine) \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing vacancy {v['id']}: {e}\")\n",
    "        print(f\"Response: {result.text}\")\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisa a cardinalidade dos requisitos que o LLM gerou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()\n",
    "db_vacancies = pd.read_sql_query(f\"\"\"\n",
    "        SELECT\n",
    "            id,\n",
    "            title,\n",
    "            llm_result\n",
    "        FROM\n",
    "            vacancies\n",
    "        WHERE\n",
    "            llm_result IS NOT NULL\n",
    "        -- LIMIT 1\n",
    "    \"\"\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "for l,v in db_vacancies.iterrows():\n",
    "    #print(l)\n",
    "    try:\n",
    "        llm_result = json.loads(v[\"llm_result\"].replace(\"'\", \"\\\"\"), strict=True)\n",
    "        llm_result[\"id\"] = v[\"id\"]\n",
    "        json_list.append(llm_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing vacancy {v['id']}: {e}\")\n",
    "        print(v[\"llm_result\"])\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(json_list).createOrReplaceTempView(\"tmp_llm_vacancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id, \n",
    "        title,\n",
    "        r.name as requirement_name,\n",
    "        r.significant_judgement as incidence\n",
    "    FROM tmp_llm_vacancy\n",
    "        LATERAL VIEW explode(requirements) AS r\n",
    "\"\"\").createOrReplaceTempView(\"tmp_llm_vacancy_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        requirement_name,\n",
    "        count(distinct id) as total\n",
    "    FROM\n",
    "        tmp_llm_vacancy_exploded\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "\"\"\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        count(distinct id) as total\n",
    "    FROM\n",
    "        tmp_llm_vacancy_exploded\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
